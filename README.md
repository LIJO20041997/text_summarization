# Text Summarizer using Transformer

+ By using the transformer model 'google/pegasus-cnn_dailymail' trained a fine-tuned model for text summarisation

+ Imported dataset from huggingface , tokenized the dataset done some prepocessing and train the model

+ To improve the performance of the model by
    + Increasing the amount of data to train
    + Longer Training by increasing the number of epochs
    + Data cleaning, data augmentation
    + And we can also use other large language models like GPT-3, BART, T5 which can give better results
